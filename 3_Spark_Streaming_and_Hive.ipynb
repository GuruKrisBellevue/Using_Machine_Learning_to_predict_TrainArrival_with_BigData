{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd0e1296-81b4-4cf3-9aad-e1e877a28556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing SparkSession \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41619e83-4caa-40f0-96f1-b2ed4fa9d94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.5.0-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/guruprasadvk10/.ivy2/cache\n",
      "The jars for the packages stored in: /home/guruprasadvk10/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-bfa3f085-f433-468b-b420-42ea50bed11f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 427ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-bfa3f085-f433-468b-b420-42ea50bed11f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/11ms)\n",
      "24/02/26 00:49:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/26 00:49:14 WARN Client: Same path resource file:///home/guruprasadvk10/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.1.jar added multiple times to distributed cache.\n",
      "24/02/26 00:49:14 WARN Client: Same path resource file:///home/guruprasadvk10/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.1.jar added multiple times to distributed cache.\n",
      "24/02/26 00:49:14 WARN Client: Same path resource file:///home/guruprasadvk10/.ivy2/jars/org.apache.kafka_kafka-clients-2.4.1.jar added multiple times to distributed cache.\n",
      "24/02/26 00:49:14 WARN Client: Same path resource file:///home/guruprasadvk10/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar added multiple times to distributed cache.\n",
      "24/02/26 00:49:14 WARN Client: Same path resource file:///home/guruprasadvk10/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar added multiple times to distributed cache.\n",
      "24/02/26 00:49:14 WARN Client: Same path resource file:///home/guruprasadvk10/.ivy2/jars/com.github.luben_zstd-jni-1.4.4-3.jar added multiple times to distributed cache.\n",
      "24/02/26 00:49:14 WARN Client: Same path resource file:///home/guruprasadvk10/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar added multiple times to distributed cache.\n",
      "24/02/26 00:49:14 WARN Client: Same path resource file:///home/guruprasadvk10/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar added multiple times to distributed cache.\n",
      "24/02/26 00:49:14 WARN Client: Same path resource file:///home/guruprasadvk10/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "# Creating a SparkSession Object\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1'). \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config('spark.sql.warehouse.dir', f'/user/warehouse'). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName('Python - Kafka and Spark Integration for Spark Streaming for CTA Project'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3836833e-fc22-4281-9c3c-5ae5fada28eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the Bootstrap servers\n",
    "kafka_bootstrap_servers = 'localhost:9092'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42e56702-30f7-4ca3-965b-a362f202f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an object for ReadStream\n",
    "df_cta = spark. \\\n",
    "  readStream. \\\n",
    "  format('kafka'). \\\n",
    "  option('kafka.bootstrap.servers', kafka_bootstrap_servers). \\\n",
    "  option('subscribe', 'cta_topic_kc'). \\\n",
    "  load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb79406f-3cce-4a69-9265-e68e467aa959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validating if the Stream is active\n",
    "df_cta.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36708e31-2cb3-4935-b62d-9701fa753778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the schema of the Stream dataframe\n",
    "df_cta.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71d14492-8eea-4069-a012-ca6316ee81d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 00:49:43 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-48d0cca2-a272-424f-8f2c-ddf52681d7cb. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/02/26 00:49:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f0ddc897c10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using console mode to create a Write Stream object to write to the stream every 30 seconds.\n",
    "df_cta.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\"). \\\n",
    "    writeStream. \\\n",
    "    outputMode(\"update\"). \\\n",
    "    format(\"console\"). \\\n",
    "    option('truncate', 'false'). \\\n",
    "    trigger(processingTime='30 seconds'). \\\n",
    "    start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc067115-3c54-40bf-b7d2-74fbf776dfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 00:49:46 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-dd3cf8d8-7e94-443f-b761-58715c136db3. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/02/26 00:49:46 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f0ddc88cf70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Using Format mode to create a Write Stream object to write to the stream. Here query name object df_cta_sql is also created\n",
    "df_cta.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\"). \\\n",
    "    writeStream. \\\n",
    "    format(\"memory\"). \\\n",
    "    queryName(\"df_cta_sql\"). \\\n",
    "    start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b14b48f8-73f1-466d-b001-900d23b6816d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----+----------------------------------------------------------------------------------------------------------+\n",
      "|key |value                                                                                                     |\n",
      "+----+----------------------------------------------------------------------------------------------------------+\n",
      "|NULL|pink,308,30114,Loop,40170,Ashland,2024-02-25T18:49:32,2024-02-25T18:51:32,0,41.88531,-87.66697            |\n",
      "|NULL|pink,310,30114,54th/Cermak,40680,Adams/Wabash,2024-02-25T18:49:06,2024-02-25T18:51:06,0,41.88574,-87.62758|\n",
      "|NULL|pink,313,30114,54th/Cermak,41030,Polk,2024-02-25T18:49:30,2024-02-25T18:51:30,0,41.88487,-87.67007        |\n",
      "|NULL|pink,314,30114,Loop,40600,Kostner,2024-02-25T18:49:37,2024-02-25T18:51:37,0,41.85192,-87.74534            |\n",
      "+----+----------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting count from the query name object\n",
    "spark.sql('SELECT count(1) FROM df_cta_sql').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bb436e6-e3fa-4aac-9422-f8d406bf1105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------------------------------------------------------------------------------------------+\n",
      "|key |value                                                                                                     |\n",
      "+----+----------------------------------------------------------------------------------------------------------+\n",
      "|NULL|pink,308,30114,Loop,40170,Ashland,2024-02-25T18:49:32,2024-02-25T18:51:32,0,41.88531,-87.66697            |\n",
      "|NULL|pink,310,30114,54th/Cermak,40680,Adams/Wabash,2024-02-25T18:49:06,2024-02-25T18:51:06,0,41.88574,-87.62758|\n",
      "|NULL|pink,313,30114,54th/Cermak,41030,Polk,2024-02-25T18:49:30,2024-02-25T18:51:30,0,41.88487,-87.67007        |\n",
      "|NULL|pink,314,30114,Loop,40600,Kostner,2024-02-25T18:49:37,2024-02-25T18:51:37,0,41.85192,-87.74534            |\n",
      "|NULL|g,008,30004,Harlem/Lake,40510,Garfield,2024-02-25T18:49:31,2024-02-25T18:53:31,0,41.78605,-87.63203       |\n",
      "|NULL|g,010,30139,Cottage Grove,41400,Roosevelt,2024-02-25T18:49:57,2024-02-25T18:51:57,0,41.87467,-87.62643    |\n",
      "|NULL|g,013,30004,Harlem/Lake,41160,Clinton,2024-02-25T18:49:26,2024-02-25T18:51:26,0,41.88574,-87.63089        |\n",
      "|NULL|g,601,30004,Harlem/Lake,41400,Roosevelt,2024-02-25T18:49:56,2024-02-25T18:50:56,0,41.86436,-87.62654      |\n",
      "|NULL|g,609,30057,Ashland/63rd,41080,47th,2024-02-25T18:48:36,2024-02-25T18:49:36,0,41.81284,-87.61892          |\n",
      "|NULL|g,610,30004,Harlem/Lake,40700,Laramie,2024-02-25T18:49:36,2024-02-25T18:51:36,0,41.88652,-87.7447         |\n",
      "|NULL|g,613,30057,Ashland/63rd,40170,Ashland,2024-02-25T18:49:25,2024-02-25T18:51:25,0,41.88466,-87.68449       |\n",
      "|NULL|g,615,30139,Cottage Grove,40700,Laramie,2024-02-25T18:49:56,2024-02-25T18:51:56,0,41.88739,-87.76565      |\n",
      "+----+----------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting data from the query name object\n",
    "spark.sql('SELECT * FROM df_cta_sql').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "123e2786-dd55-44c4-8cfb-92aec8e6e6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing sql functions for processing\n",
    "from pyspark.sql.functions import lit, date_format, to_date, split, substring,unix_timestamp, from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee0a9cd1-f48a-48ea-8397-208cf652e3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 00:50:12 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6cfa1cc1-4a47-4b99-b005-b3da333857e0. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/02/26 00:50:12 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f0ddc8b1490>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Casting the columns as String and Adding new columns to extract Year, month and date information\n",
    "df_cta.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\"). \\\n",
    "    withColumn('route_color', split('value', ',')[0]). \\\n",
    "    withColumn('transit_date', to_date(split('value', ',')[6], \"yyyy-MM-dd'T'HH:mm:ss\")). \\\n",
    "    withColumn('year', date_format('transit_date', 'yyyy')). \\\n",
    "    withColumn('month', date_format('transit_date', 'MM')). \\\n",
    "    withColumn('dayofmonth', date_format('transit_date', 'dd')). \\\n",
    "    writeStream. \\\n",
    "    format(\"memory\"). \\\n",
    "    queryName(\"df_cta_sql1\"). \\\n",
    "    start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f939949e-2a12-44da-aee1-745950592396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------------------------------------------------------------------------+-----------+------------+----+-----+----------+\n",
      "|key |value                                                                                            |route_color|transit_date|year|month|dayofmonth|\n",
      "+----+-------------------------------------------------------------------------------------------------+-----------+------------+----+-----+----------+\n",
      "|NULL|brn,409,30249,Loop,41480,Western,2024-02-25T18:49:46,2024-02-25T18:50:46,0,41.96624,-87.69073    |brn        |2024-02-25  |2024|02   |25        |\n",
      "|NULL|brn,410,30249,Loop,40530,Diversey,2024-02-25T18:49:41,2024-02-25T18:51:41,0,41.93975,-87.65338   |brn        |2024-02-25  |2024|02   |25        |\n",
      "|NULL|brn,413,30249,Kimball,40710,Chicago,2024-02-25T18:49:58,2024-02-25T18:50:58,0,41.89678,-87.63595 |brn        |2024-02-25  |2024|02   |25        |\n",
      "|NULL|brn,414,30249,Kimball,40040,Quincy,2024-02-25T18:49:55,2024-02-25T18:50:55,0,41.88309,-87.63386  |brn        |2024-02-25  |2024|02   |25        |\n",
      "|NULL|brn,415,30249,Kimball,41500,Montrose,2024-02-25T18:49:48,2024-02-25T18:50:48,0,41.95813,-87.67506|brn        |2024-02-25  |2024|02   |25        |\n",
      "+----+-------------------------------------------------------------------------------------------------+-----------+------------+----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting sample rows \n",
    "spark.sql('SELECT * FROM df_cta_sql1').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "224bd6a2-1fb6-4b4f-adb1-340b0532eb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /final_project_hdfs\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+----+------------------------------------------------------------------------------------------------------+\n",
      "|key |value                                                                                                 |\n",
      "+----+------------------------------------------------------------------------------------------------------+\n",
      "|NULL|g,008,30004,Harlem/Lake,40510,Garfield,2024-02-25T18:49:31,2024-02-25T18:53:31,0,41.78605,-87.63203   |\n",
      "|NULL|g,010,30139,Cottage Grove,41400,Roosevelt,2024-02-25T18:49:57,2024-02-25T18:51:57,0,41.87467,-87.62643|\n",
      "|NULL|g,013,30004,Harlem/Lake,41160,Clinton,2024-02-25T18:49:26,2024-02-25T18:51:26,0,41.88574,-87.63089    |\n",
      "|NULL|g,601,30004,Harlem/Lake,41400,Roosevelt,2024-02-25T18:49:56,2024-02-25T18:50:56,0,41.86436,-87.62654  |\n",
      "|NULL|g,609,30057,Ashland/63rd,41080,47th,2024-02-25T18:48:36,2024-02-25T18:49:36,0,41.81284,-87.61892      |\n",
      "|NULL|g,610,30004,Harlem/Lake,40700,Laramie,2024-02-25T18:49:36,2024-02-25T18:51:36,0,41.88652,-87.7447     |\n",
      "|NULL|g,613,30057,Ashland/63rd,40170,Ashland,2024-02-25T18:49:25,2024-02-25T18:51:25,0,41.88466,-87.68449   |\n",
      "|NULL|g,615,30139,Cottage Grove,40700,Laramie,2024-02-25T18:49:56,2024-02-25T18:51:56,0,41.88739,-87.76565  |\n",
      "|NULL|brn,409,30249,Loop,41480,Western,2024-02-25T18:49:46,2024-02-25T18:50:46,0,41.96624,-87.69073         |\n",
      "|NULL|brn,410,30249,Loop,40530,Diversey,2024-02-25T18:49:41,2024-02-25T18:51:41,0,41.93975,-87.65338        |\n",
      "|NULL|brn,413,30249,Kimball,40710,Chicago,2024-02-25T18:49:58,2024-02-25T18:50:58,0,41.89678,-87.63595      |\n",
      "|NULL|brn,414,30249,Kimball,40040,Quincy,2024-02-25T18:49:55,2024-02-25T18:50:55,0,41.88309,-87.63386       |\n",
      "|NULL|brn,415,30249,Kimball,41500,Montrose,2024-02-25T18:49:48,2024-02-25T18:50:48,0,41.95813,-87.67506     |\n",
      "|NULL|g,008,30004,Harlem/Lake,40510,Garfield,2024-02-25T18:50:04,2024-02-25T18:54:04,0,41.78681,-87.62988   |\n",
      "|NULL|g,010,30139,Cottage Grove,41400,Roosevelt,2024-02-25T18:50:15,2024-02-25T18:52:15,0,41.87332,-87.62679|\n",
      "|NULL|g,013,30004,Harlem/Lake,41160,Clinton,2024-02-25T18:50:17,2024-02-25T18:52:17,0,41.88573,-87.63288    |\n",
      "|NULL|g,601,30004,Harlem/Lake,41400,Roosevelt,2024-02-25T18:50:18,2024-02-25T18:51:18,0,41.86683,-87.62658  |\n",
      "|NULL|g,609,30057,Ashland/63rd,41080,47th,2024-02-25T18:48:36,2024-02-25T18:49:36,0,41.81284,-87.61892      |\n",
      "|NULL|g,610,30004,Harlem/Lake,40700,Laramie,2024-02-25T18:49:36,2024-02-25T18:51:36,0,41.88652,-87.7447     |\n",
      "|NULL|g,613,30057,Ashland/63rd,40170,Ashland,2024-02-25T18:50:08,2024-02-25T18:52:08,0,41.88482,-87.68058   |\n",
      "+----+------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R -skipTrash /final_project_hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ece30-b989-464b-9a54-9635677e5a83",
   "metadata": {},
   "source": [
    "### Writing Spark Streaming data to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02da1f34-c390-4f8c-bc82-887c38bc6642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 00:50:36 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f0ddc8b1d00>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Writing to HDFS and partitioning the data using Year, Month and Day columns. Data is written to HDFS every 30 seconds from the Spark stream.\n",
    "df_cta.selectExpr(\"CAST(value AS STRING)\"). \\\n",
    "    withColumn('route_color', split('value', ',')[0]). \\\n",
    "    withColumn('transit_date', to_date(split('value', ',')[6], \"yyyy-MM-dd'T'HH:mm:ss\")). \\\n",
    "    withColumn('year', date_format('transit_date', 'yyyy')). \\\n",
    "    withColumn('month', date_format('transit_date', 'MM')). \\\n",
    "    withColumn('dayofmonth', date_format('transit_date', 'dd')). \\\n",
    "    writeStream. \\\n",
    "    partitionBy('year', 'month', 'dayofmonth'). \\\n",
    "    format('csv'). \\\n",
    "    option(\"checkpointLocation\", '/final_project_hdfs/checkpoint'). \\\n",
    "    option('path', '/final_project_hdfs/data'). \\\n",
    "    option('header',True). \\\n",
    "    option('sep','|'). \\\n",
    "    trigger(processingTime='30 seconds'). \\\n",
    "    start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26838927-02d0-4cf1-b484-ad184a410f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxr-xr-x   - guruprasadvk10 supergroup          0 2024-02-26 00:50 /final_project_hdfs/checkpoint\n",
      "drwxr-xr-x   - guruprasadvk10 supergroup          0 2024-02-26 00:50 /final_project_hdfs/data\n"
     ]
    }
   ],
   "source": [
    "# Validating the HDFS directories\n",
    "!hdfs dfs -ls /final_project_hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7344cfd3-c156-4805-a7b1-9c921bdbafd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - guruprasadvk10 supergroup          0 2024-02-26 00:51 /final_project_hdfs/data/year=2024/month=02\n",
      "drwxr-xr-x   - guruprasadvk10 supergroup          0 2024-02-26 00:51 /final_project_hdfs/data/year=2024/month=02/dayofmonth=25\n",
      "-rw-r--r--   1 guruprasadvk10 supergroup       1891 2024-02-26 00:51 /final_project_hdfs/data/year=2024/month=02/dayofmonth=25/part-00000-559278f9-7ecf-4adc-9990-1b132b4bb10f.c000.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -R /final_project_hdfs/data/year=2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9a76a93-f787-4a6d-b4b4-3488692b63cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwxr-xr-x   - guruprasadvk10 supergroup          0 2024-02-26 00:51 /final_project_hdfs/checkpoint/commits\n",
      "-rw-r--r--   1 guruprasadvk10 supergroup         45 2024-02-26 00:50 /final_project_hdfs/checkpoint/metadata\n",
      "drwxr-xr-x   - guruprasadvk10 supergroup          0 2024-02-26 00:51 /final_project_hdfs/checkpoint/offsets\n",
      "drwxr-xr-x   - guruprasadvk10 supergroup          0 2024-02-26 00:50 /final_project_hdfs/checkpoint/sources\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /final_project_hdfs/checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24caca39-b8a5-44c9-a1ec-6d835c51fe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000v1\n",
      "{\"cta_topic_kc\":{\"0\":204}}"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /final_project_hdfs/checkpoint/sources/0/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b788b5ac-cede-4c58-8b9a-ed6068fd3a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 guruprasadvk10 supergroup        667 2024-02-26 00:50 /final_project_hdfs/checkpoint/offsets/0\n",
      "-rw-r--r--   1 guruprasadvk10 supergroup        667 2024-02-26 00:51 /final_project_hdfs/checkpoint/offsets/1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /final_project_hdfs/checkpoint/offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fdc14b8-1e85-4332-96eb-97a9c31575d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\n",
      "{\"batchWatermarkMs\":0,\"batchTimestampMs\":1708908636653,\"conf\":{\"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\",\"spark.sql.streaming.join.stateFormatVersion\":\"2\",\"spark.sql.streaming.stateStore.compression.codec\":\"lz4\",\"spark.sql.streaming.stateStore.rocksdb.formatVersion\":\"5\",\"spark.sql.streaming.statefulOperator.useStrictDistribution\":\"true\",\"spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion\":\"2\",\"spark.sql.streaming.multipleWatermarkPolicy\":\"min\",\"spark.sql.streaming.aggregation.stateFormatVersion\":\"2\",\"spark.sql.shuffle.partitions\":\"200\"}}\n",
      "{\"cta_topic_kc\":{\"0\":204}}-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+----+--------------------------------------------------------------------------------------------------------+\n",
      "|key |value                                                                                                   |\n",
      "+----+--------------------------------------------------------------------------------------------------------+\n",
      "|NULL|org,708,30182,Midway,41400,Roosevelt,2024-02-25T18:50:24,2024-02-25T18:53:24,0,41.8795,-87.6261         |\n",
      "|NULL|org,710,30182,Loop,41130,Halsted,2024-02-25T18:50:28,2024-02-25T18:52:28,0,41.83957,-87.66485           |\n",
      "|NULL|org,711,30182,Midway,40930,Midway,2024-02-25T18:50:37,2024-02-25T18:51:37,0,41.79572,-87.735            |\n",
      "|NULL|org,712,30182,Midway,40310,Western,2024-02-25T18:50:52,2024-02-25T18:51:52,0,41.81395,-87.67972         |\n",
      "|NULL|org,713,30182,Midway,40730,Washington/Wells,2024-02-25T18:50:41,2024-02-25T18:51:41,0,41.87872,-87.63374|\n",
      "|NULL|org,714,30182,Loop,40960,Pulaski,2024-02-25T18:50:34,2024-02-25T18:51:34,0,41.79497,-87.73636           |\n",
      "|NULL|g,008,30004,Harlem/Lake,40510,Garfield,2024-02-25T18:50:46,2024-02-25T18:52:46,0,41.7869,-87.62415      |\n",
      "|NULL|g,010,30139,Cottage Grove,41400,Roosevelt,2024-02-25T18:50:56,2024-02-25T18:52:56,0,41.87259,-87.62677  |\n",
      "|NULL|g,013,30004,Harlem/Lake,41160,Clinton,2024-02-25T18:50:52,2024-02-25T18:51:52,0,41.88572,-87.63614      |\n",
      "|NULL|g,601,30004,Harlem/Lake,40680,Adams/Wabash,2024-02-25T18:50:28,2024-02-25T18:54:28,0,41.86744,-87.62659 |\n",
      "|NULL|g,609,30057,Ashland/63rd,40510,Garfield,2024-02-25T18:50:40,2024-02-25T18:51:40,0,41.80209,-87.6185     |\n",
      "|NULL|g,610,30004,Harlem/Lake,40700,Laramie,2024-02-25T18:50:42,2024-02-25T18:51:42,0,41.8867,-87.74717       |\n",
      "|NULL|g,613,30057,Ashland/63rd,40170,Ashland,2024-02-25T18:50:08,2024-02-25T18:52:08,0,41.88482,-87.68058     |\n",
      "|NULL|g,615,30139,Cottage Grove,40700,Laramie,2024-02-25T18:50:57,2024-02-25T18:51:57,0,41.88744,-87.76209    |\n",
      "|NULL|org,708,30182,Midway,41400,Roosevelt,2024-02-25T18:51:14,2024-02-25T18:53:14,0,41.87652,-87.62601       |\n",
      "|NULL|org,710,30182,Loop,41130,Halsted,2024-02-25T18:51:16,2024-02-25T18:52:16,0,41.84101,-87.66183           |\n",
      "|NULL|org,711,30182,Midway,40930,Midway,2024-02-25T18:51:16,2024-02-25T18:52:16,0,41.79141,-87.73813          |\n",
      "|NULL|org,712,30182,Midway,40310,Western,2024-02-25T18:51:17,2024-02-25T18:52:17,0,41.80903,-87.67959         |\n",
      "|NULL|org,713,30182,Midway,40730,Washington/Wells,2024-02-25T18:51:10,2024-02-25T18:52:10,0,41.87945,-87.63376|\n",
      "|NULL|org,714,30182,Loop,40960,Pulaski,2024-02-25T18:50:59,2024-02-25T18:51:59,0,41.79854,-87.72789           |\n",
      "+----+--------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /final_project_hdfs/checkpoint/offsets/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "188c8e9d-fd49-4828-a840-0bc47a821d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating the data written in HDFS by creating spark dataframe\n",
    "df_from_hdfs1=spark.read.csv(\"/final_project_hdfs/data/\",sep=\"|\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5707831-5b10-42e4-b340-1baad64c8c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- route_color: string (nullable = true)\n",
      " |-- transit_date: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- dayofmonth: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking schema\n",
    "df_from_hdfs1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bc80d8a-884c-4281-a776-a9d454e46c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------+-----------+------------+----+-----+----------+\n",
      "|value                                                                                                   |route_color|transit_date|year|month|dayofmonth|\n",
      "+--------------------------------------------------------------------------------------------------------+-----------+------------+----+-----+----------+\n",
      "|org,708,30182,Midway,41400,Roosevelt,2024-02-25T18:50:24,2024-02-25T18:53:24,0,41.8795,-87.6261         |org        |2024-02-25  |2024|2    |25        |\n",
      "|org,710,30182,Loop,41130,Halsted,2024-02-25T18:50:28,2024-02-25T18:52:28,0,41.83957,-87.66485           |org        |2024-02-25  |2024|2    |25        |\n",
      "|org,711,30182,Midway,40930,Midway,2024-02-25T18:50:37,2024-02-25T18:51:37,0,41.79572,-87.735            |org        |2024-02-25  |2024|2    |25        |\n",
      "|org,712,30182,Midway,40310,Western,2024-02-25T18:50:52,2024-02-25T18:51:52,0,41.81395,-87.67972         |org        |2024-02-25  |2024|2    |25        |\n",
      "|org,713,30182,Midway,40730,Washington/Wells,2024-02-25T18:50:41,2024-02-25T18:51:41,0,41.87872,-87.63374|org        |2024-02-25  |2024|2    |25        |\n",
      "|org,714,30182,Loop,40960,Pulaski,2024-02-25T18:50:34,2024-02-25T18:51:34,0,41.79497,-87.73636           |org        |2024-02-25  |2024|2    |25        |\n",
      "|g,008,30004,Harlem/Lake,40510,Garfield,2024-02-25T18:50:46,2024-02-25T18:52:46,0,41.7869,-87.62415      |g          |2024-02-25  |2024|2    |25        |\n",
      "|g,010,30139,Cottage Grove,41400,Roosevelt,2024-02-25T18:50:56,2024-02-25T18:52:56,0,41.87259,-87.62677  |g          |2024-02-25  |2024|2    |25        |\n",
      "|g,013,30004,Harlem/Lake,41160,Clinton,2024-02-25T18:50:52,2024-02-25T18:51:52,0,41.88572,-87.63614      |g          |2024-02-25  |2024|2    |25        |\n",
      "|g,601,30004,Harlem/Lake,40680,Adams/Wabash,2024-02-25T18:50:28,2024-02-25T18:54:28,0,41.86744,-87.62659 |g          |2024-02-25  |2024|2    |25        |\n",
      "|g,609,30057,Ashland/63rd,40510,Garfield,2024-02-25T18:50:40,2024-02-25T18:51:40,0,41.80209,-87.6185     |g          |2024-02-25  |2024|2    |25        |\n",
      "|g,610,30004,Harlem/Lake,40700,Laramie,2024-02-25T18:50:42,2024-02-25T18:51:42,0,41.8867,-87.74717       |g          |2024-02-25  |2024|2    |25        |\n",
      "|g,613,30057,Ashland/63rd,40170,Ashland,2024-02-25T18:50:08,2024-02-25T18:52:08,0,41.88482,-87.68058     |g          |2024-02-25  |2024|2    |25        |\n",
      "|g,615,30139,Cottage Grove,40700,Laramie,2024-02-25T18:50:57,2024-02-25T18:51:57,0,41.88744,-87.76209    |g          |2024-02-25  |2024|2    |25        |\n",
      "|org,708,30182,Midway,41400,Roosevelt,2024-02-25T18:51:14,2024-02-25T18:53:14,0,41.87652,-87.62601       |org        |2024-02-25  |2024|2    |25        |\n",
      "|org,710,30182,Loop,41130,Halsted,2024-02-25T18:51:16,2024-02-25T18:52:16,0,41.84101,-87.66183           |org        |2024-02-25  |2024|2    |25        |\n",
      "|org,711,30182,Midway,40930,Midway,2024-02-25T18:51:16,2024-02-25T18:52:16,0,41.79141,-87.73813          |org        |2024-02-25  |2024|2    |25        |\n",
      "|org,712,30182,Midway,40310,Western,2024-02-25T18:51:17,2024-02-25T18:52:17,0,41.80903,-87.67959         |org        |2024-02-25  |2024|2    |25        |\n",
      "|org,713,30182,Midway,40730,Washington/Wells,2024-02-25T18:51:10,2024-02-25T18:52:10,0,41.87945,-87.63376|org        |2024-02-25  |2024|2    |25        |\n",
      "|org,714,30182,Loop,40960,Pulaski,2024-02-25T18:50:59,2024-02-25T18:51:59,0,41.79854,-87.72789           |org        |2024-02-25  |2024|2    |25        |\n",
      "+--------------------------------------------------------------------------------------------------------+-----------+------------+----+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CHecking sample rows\n",
    "df_from_hdfs1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec0e569-8f7f-4095-9f6b-1c6bb0774c66",
   "metadata": {},
   "source": [
    "### Creating tables in HIVE and querying the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93b0522d-b763-44fa-ba88-ea19c3f32cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 00:51:44 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/02/26 00:51:44 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>namespace</th></tr>\n",
       "<tr><td>cta_db</td></tr>\n",
       "<tr><td>default</td></tr>\n",
       "<tr><td>retail_db</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+\n",
       "|namespace|\n",
       "+---------+\n",
       "|   cta_db|\n",
       "|  default|\n",
       "|retail_db|\n",
       "+---------+"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking list of databases in Hive\n",
    "spark.sql(\"show databases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcb370c3-df49-486f-a40f-1ea7d982a35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 00:51:48 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the cta_db created for this project\n",
    "spark.sql(\"use cta_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "726db164-7d9b-4b4c-8476-3f382193fad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the table cta_data is it exists already\n",
    "spark.sql(\"DROP TABLE IF EXISTS cta_data;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f73bfb8-79a9-4ba5-ac37-82bc485424f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 00:51:54 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/02/26 00:51:54 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/02/26 00:51:54 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/02/26 00:51:54 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/02/26 00:51:54 WARN HiveMetaStore: Location: hdfs://localhost:9000/user/hive/warehouse/cta_db.db/cta_data specified for non-external table:cta_data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Hive table structure\n",
    "spark.sql(\"CREATE TABLE cta_data (    value STRING,    route_color  STRING,    transit_date STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|';\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45cbca16-2e2b-465e-b162-f74043ff6395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+----+----------------------------------------------------------------------------------------------------------+\n",
      "|key |value                                                                                                     |\n",
      "+----+----------------------------------------------------------------------------------------------------------+\n",
      "|NULL|brn,409,30249,Loop,40090,Damen,2024-02-25T18:51:01,2024-02-25T18:52:01,0,41.96634,-87.68351               |\n",
      "|NULL|brn,410,30249,Loop,40530,Diversey,2024-02-25T18:51:00,2024-02-25T18:52:00,0,41.93602,-87.65327            |\n",
      "|NULL|brn,413,30249,Kimball,40800,Sedgwick,2024-02-25T18:50:51,2024-02-25T18:53:51,0,41.89678,-87.63595         |\n",
      "|NULL|brn,414,30249,Kimball,40160,LaSalle/Van Buren,2024-02-25T18:51:00,2024-02-25T18:53:00,0,41.87872,-87.63374|\n",
      "|NULL|brn,415,30249,Kimball,40090,Damen,2024-02-25T18:50:24,2024-02-25T18:52:24,0,41.96175,-87.67517            |\n",
      "|NULL|pink,308,30114,Loop,40170,Ashland,2024-02-25T18:50:35,2024-02-25T18:52:35,0,41.88531,-87.66697            |\n",
      "|NULL|pink,310,30114,54th/Cermak,40680,Adams/Wabash,2024-02-25T18:51:36,2024-02-25T18:52:36,0,41.88202,-87.62616|\n",
      "|NULL|pink,313,30114,54th/Cermak,40830,18th,2024-02-25T18:51:18,2024-02-25T18:53:18,0,41.87155,-87.66954        |\n",
      "|NULL|pink,314,30114,Loop,40600,Kostner,2024-02-25T18:51:37,2024-02-25T18:52:37,0,41.85374,-87.73441            |\n",
      "|NULL|pink,308,30114,Loop,40170,Ashland,2024-02-25T18:50:35,2024-02-25T18:52:35,0,41.88531,-87.66697            |\n",
      "|NULL|pink,310,30114,54th/Cermak,40680,Adams/Wabash,2024-02-25T18:51:36,2024-02-25T18:52:36,0,41.88202,-87.62616|\n",
      "|NULL|pink,313,30114,54th/Cermak,40830,18th,2024-02-25T18:51:18,2024-02-25T18:53:18,0,41.87155,-87.66954        |\n",
      "|NULL|pink,314,30114,Loop,40600,Kostner,2024-02-25T18:51:37,2024-02-25T18:52:37,0,41.85374,-87.73441            |\n",
      "+----+----------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading data into Hive table using HDFS data\n",
    "spark.sql(\"LOAD DATA INPATH '/final_project_hdfs/data/year=2024/month=02/dayofmonth=25/' INTO TABLE cta_data;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93cd3e26-7ff7-4cb0-a2d3-0e325be24a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>value</th><th>route_color</th><th>transit_date</th></tr>\n",
       "<tr><td>value</td><td>route_color</td><td>transit_date</td></tr>\n",
       "<tr><td>org,708,30182,Mid...</td><td>org</td><td>2024-02-25</td></tr>\n",
       "<tr><td>org,710,30182,Loo...</td><td>org</td><td>2024-02-25</td></tr>\n",
       "<tr><td>org,711,30182,Mid...</td><td>org</td><td>2024-02-25</td></tr>\n",
       "<tr><td>org,712,30182,Mid...</td><td>org</td><td>2024-02-25</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+-----------+------------+\n",
       "|               value|route_color|transit_date|\n",
       "+--------------------+-----------+------------+\n",
       "|               value|route_color|transit_date|\n",
       "|org,708,30182,Mid...|        org|  2024-02-25|\n",
       "|org,710,30182,Loo...|        org|  2024-02-25|\n",
       "|org,711,30182,Mid...|        org|  2024-02-25|\n",
       "|org,712,30182,Mid...|        org|  2024-02-25|\n",
       "+--------------------+-----------+------------+"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting sample rows from cta_data tables\n",
    "spark.sql(\"SELECT * FROM cta_data LIMIT 5;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2366635-676f-4277-9eb2-a85ea65ff8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count(1)</th></tr>\n",
       "<tr><td>38</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+\n",
       "|count(1)|\n",
       "+--------+\n",
       "|      38|\n",
       "+--------+"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting count of rows from cta_data tables\n",
    "spark.sql(\"SELECT count(1) FROM cta_data;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5866a44d-6576-4b8a-972e-22d534e49a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>value</th><th>route_color</th><th>transit_date</th></tr>\n",
       "<tr><td>g,008,30004,Harle...</td><td>g</td><td>2024-02-25</td></tr>\n",
       "<tr><td>g,010,30139,Cotta...</td><td>g</td><td>2024-02-25</td></tr>\n",
       "<tr><td>g,013,30004,Harle...</td><td>g</td><td>2024-02-25</td></tr>\n",
       "<tr><td>g,601,30004,Harle...</td><td>g</td><td>2024-02-25</td></tr>\n",
       "<tr><td>g,609,30057,Ashla...</td><td>g</td><td>2024-02-25</td></tr>\n",
       "<tr><td>g,610,30004,Harle...</td><td>g</td><td>2024-02-25</td></tr>\n",
       "<tr><td>g,613,30057,Ashla...</td><td>g</td><td>2024-02-25</td></tr>\n",
       "<tr><td>g,615,30139,Cotta...</td><td>g</td><td>2024-02-25</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+-----------+------------+\n",
       "|               value|route_color|transit_date|\n",
       "+--------------------+-----------+------------+\n",
       "|g,008,30004,Harle...|          g|  2024-02-25|\n",
       "|g,010,30139,Cotta...|          g|  2024-02-25|\n",
       "|g,013,30004,Harle...|          g|  2024-02-25|\n",
       "|g,601,30004,Harle...|          g|  2024-02-25|\n",
       "|g,609,30057,Ashla...|          g|  2024-02-25|\n",
       "|g,610,30004,Harle...|          g|  2024-02-25|\n",
       "|g,613,30057,Ashla...|          g|  2024-02-25|\n",
       "|g,615,30139,Cotta...|          g|  2024-02-25|\n",
       "+--------------------+-----------+------------+"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM cta_data where route_color='g';\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
